{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Israel Rodr√≠guez\n",
    "- Benjam√≠n Torrealba    \n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: https://github.com/BnjmnNicholas/MDS7202-2023-2\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('pdf') # Para solucionar el error de guardar las img en pdf, Esto genera que no funcione el plt.show()\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import os \n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "from sklearn.preprocessing import FunctionTransformer, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, Birch, AffinityPropagation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Team</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Games</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Event</th>\n",
       "      <th>Medal</th>\n",
       "      <th>age-height-weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98308</td>\n",
       "      <td>Khalid Raghib</td>\n",
       "      <td>M</td>\n",
       "      <td>Morocco</td>\n",
       "      <td>MAR</td>\n",
       "      <td>1992 Summer</td>\n",
       "      <td>1992</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Football</td>\n",
       "      <td>Football Men's Football</td>\n",
       "      <td>None</td>\n",
       "      <td>22.0(180.0?70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121701</td>\n",
       "      <td>Mrioara Trac (-Curelea)</td>\n",
       "      <td>F</td>\n",
       "      <td>Romania</td>\n",
       "      <td>ROU</td>\n",
       "      <td>1984 Summer</td>\n",
       "      <td>1984</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Rowing</td>\n",
       "      <td>Rowing Women's Coxed Eights</td>\n",
       "      <td>Silver</td>\n",
       "      <td>21.0?172.0?68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133023</td>\n",
       "      <td>Martin Laumann Ylven</td>\n",
       "      <td>M</td>\n",
       "      <td>Norway</td>\n",
       "      <td>NOR</td>\n",
       "      <td>2010 Winter</td>\n",
       "      <td>2010</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Vancouver</td>\n",
       "      <td>Ice Hockey</td>\n",
       "      <td>Ice Hockey Men's Ice Hockey</td>\n",
       "      <td>None</td>\n",
       "      <td>21.0(190.0?92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72524</td>\n",
       "      <td>Wilhelm Bernhard Adolf Emil \"Willy\" Ltzow</td>\n",
       "      <td>M</td>\n",
       "      <td>Germany</td>\n",
       "      <td>GER</td>\n",
       "      <td>1912 Summer</td>\n",
       "      <td>1912</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Stockholm</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Swimming Men's 400 metres Breaststroke</td>\n",
       "      <td>None</td>\n",
       "      <td>19.0(nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75751</td>\n",
       "      <td>Jess Martnez Tejeda</td>\n",
       "      <td>M</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>MEX</td>\n",
       "      <td>1996 Summer</td>\n",
       "      <td>1996</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Boxing</td>\n",
       "      <td>Boxing Men's Light-Flyweight</td>\n",
       "      <td>None</td>\n",
       "      <td>20.0(163.0?48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27107</th>\n",
       "      <td>64807</td>\n",
       "      <td>Istvan Kulcsar</td>\n",
       "      <td>M</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>SUI</td>\n",
       "      <td>1972 Summer</td>\n",
       "      <td>1972</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Fencing</td>\n",
       "      <td>Fencing Men's Sabre, Individual</td>\n",
       "      <td>None</td>\n",
       "      <td>35.0?171.0?65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27108</th>\n",
       "      <td>94402</td>\n",
       "      <td>Xavier Phelippon</td>\n",
       "      <td>M</td>\n",
       "      <td>France</td>\n",
       "      <td>FRA</td>\n",
       "      <td>1988 Summer</td>\n",
       "      <td>1988</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>Sailing</td>\n",
       "      <td>Sailing Mixed Three Person Keelboat</td>\n",
       "      <td>None</td>\n",
       "      <td>29.0:190.0?88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27109</th>\n",
       "      <td>99800</td>\n",
       "      <td>Aarne Eemeli Reini</td>\n",
       "      <td>M</td>\n",
       "      <td>Finland</td>\n",
       "      <td>FIN</td>\n",
       "      <td>1932 Summer</td>\n",
       "      <td>1932</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Wrestling</td>\n",
       "      <td>Wrestling Men's Lightweight, Greco-Roman</td>\n",
       "      <td>None</td>\n",
       "      <td>25.0?nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27110</th>\n",
       "      <td>59573</td>\n",
       "      <td>Habib Ali Kiddie</td>\n",
       "      <td>M</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>PAK</td>\n",
       "      <td>1952 Summer</td>\n",
       "      <td>1952</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Hockey</td>\n",
       "      <td>Hockey Men's Hockey</td>\n",
       "      <td>None</td>\n",
       "      <td>22.0?173.0?60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27111</th>\n",
       "      <td>135253</td>\n",
       "      <td>Zdenko Zorko</td>\n",
       "      <td>M</td>\n",
       "      <td>Yugoslavia</td>\n",
       "      <td>YUG</td>\n",
       "      <td>1976 Summer</td>\n",
       "      <td>1976</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>Handball</td>\n",
       "      <td>Handball Men's Handball</td>\n",
       "      <td>None</td>\n",
       "      <td>25.0*187.0?80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27112 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                       Name Sex         Team  \\\n",
       "0       98308                              Khalid Raghib   M      Morocco   \n",
       "1      121701                    Mrioara Trac (-Curelea)   F      Romania   \n",
       "2      133023                       Martin Laumann Ylven   M       Norway   \n",
       "3       72524  Wilhelm Bernhard Adolf Emil \"Willy\" Ltzow   M      Germany   \n",
       "4       75751                        Jess Martnez Tejeda   M       Mexico   \n",
       "...       ...                                        ...  ..          ...   \n",
       "27107   64807                             Istvan Kulcsar   M  Switzerland   \n",
       "27108   94402                           Xavier Phelippon   M       France   \n",
       "27109   99800                         Aarne Eemeli Reini   M      Finland   \n",
       "27110   59573                           Habib Ali Kiddie   M     Pakistan   \n",
       "27111  135253                               Zdenko Zorko   M   Yugoslavia   \n",
       "\n",
       "       NOC        Games  Year  Season         City       Sport  \\\n",
       "0      MAR  1992 Summer  1992  Summer    Barcelona    Football   \n",
       "1      ROU  1984 Summer  1984  Summer  Los Angeles      Rowing   \n",
       "2      NOR  2010 Winter  2010  Winter    Vancouver  Ice Hockey   \n",
       "3      GER  1912 Summer  1912  Summer    Stockholm    Swimming   \n",
       "4      MEX  1996 Summer  1996  Summer      Atlanta      Boxing   \n",
       "...    ...          ...   ...     ...          ...         ...   \n",
       "27107  SUI  1972 Summer  1972  Summer       Munich     Fencing   \n",
       "27108  FRA  1988 Summer  1988  Summer        Seoul     Sailing   \n",
       "27109  FIN  1932 Summer  1932  Summer  Los Angeles   Wrestling   \n",
       "27110  PAK  1952 Summer  1952  Summer     Helsinki      Hockey   \n",
       "27111  YUG  1976 Summer  1976  Summer     Montreal    Handball   \n",
       "\n",
       "                                          Event   Medal age-height-weight  \n",
       "0                       Football Men's Football    None   22.0(180.0?70.0  \n",
       "1                   Rowing Women's Coxed Eights  Silver   21.0?172.0?68.0  \n",
       "2                   Ice Hockey Men's Ice Hockey    None   21.0(190.0?92.0  \n",
       "3        Swimming Men's 400 metres Breaststroke    None      19.0(nan?nan  \n",
       "4                  Boxing Men's Light-Flyweight    None   20.0(163.0?48.0  \n",
       "...                                         ...     ...               ...  \n",
       "27107           Fencing Men's Sabre, Individual    None   35.0?171.0?65.0  \n",
       "27108       Sailing Mixed Three Person Keelboat    None   29.0:190.0?88.0  \n",
       "27109  Wrestling Men's Lightweight, Greco-Roman    None      25.0?nan?nan  \n",
       "27110                       Hockey Men's Hockey    None   22.0?173.0?60.0  \n",
       "27111                   Handball Men's Handball    None   25.0*187.0?80.0  \n",
       "\n",
       "[27112 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_parquet('olimpiadas.parquet')\n",
    "dataset = dataset.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
    "dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante:\n",
    "\n",
    "- Modificar la funci√≥n que transforma las columnas no atomicas para que los nombres que pone a las nuevas columnas tengan relaci√≥n con la columna original, agregando un subindice.\n",
    "- Falta levantar la alerta de anomalias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "\n",
    "def detectar_outliers(data_column):\n",
    "    Q1 = data_column.quantile(0.25)\n",
    "    Q3 = data_column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = (data_column < lower_bound) | (data_column > upper_bound)\n",
    "    return any(outliers)\n",
    "\n",
    "def cramer_v(x, y):\n",
    "\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente V de Cramer entre dos variables categ√≥ricas.\n",
    "\n",
    "    Parameters:\n",
    "    x (pandas.Series): Una columna de un DataFrame de pandas.\n",
    "    y (pandas.Series): Una columna de un DataFrame de pandas.\n",
    "\n",
    "    Returns:\n",
    "    float: El coeficiente V de Cramer entre las dos variables categ√≥ricas.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n",
    "\n",
    "def detectar_columnas_no_atomicas(dataframe):\n",
    "    columnas_no_atomicas = []\n",
    "    patron = re.compile(r'^\\d+(\\.\\d+)?([\\*\\(\\)\\?]\\d+(\\.\\d+)?)+$')\n",
    "\n",
    "    for columna in dataframe.columns:\n",
    "        if any(dataframe[columna].apply(lambda x: True if re.match(patron, str(x)) and ' ' not in str(x) else False)):\n",
    "            columnas_no_atomicas.append(columna)\n",
    "\n",
    "    return columnas_no_atomicas\n",
    "\n",
    "def transformar_columna(datos):\n",
    "    \"\"\"\n",
    "    Transforma una columna con datos no at√≥micos en un DataFrame de pandas en m√∫ltiples columnas.\n",
    "    \n",
    "    Parameters:\n",
    "    datos (pandas.Series): Una columna de un DataFrame de pandas que contiene datos no at√≥micos.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Un DataFrame que contiene los datos separados en columnas seg√∫n el patr√≥n identificado.\n",
    "\n",
    "    \"\"\"\n",
    "    # nombre de la columna\n",
    "    nombre_columna = datos.name\n",
    "\n",
    "    # Definir el patr√≥n de separaci√≥n usando expresiones regulares\n",
    "    pattern = re.compile(r'[\\s,:/\\-?*()]+')\n",
    "\n",
    "    # Dividir los datos seg√∫n el patr√≥n de separaci√≥n y convertirlos en una lista de listas\n",
    "    new_data = [re.split(pattern, d) for d in datos]\n",
    "    \n",
    "    # remplazamos los strings 'nan' por np.nan\n",
    "    new_data = [[np.nan if d == 'nan' else d for d in l] for l in new_data]\n",
    "    \n",
    "    # si es tipo numerico convertimos a float\n",
    "    new_data = [[float(d) if d != '' else np.nan for d in l] for l in new_data]\n",
    "\n",
    "    # Determinar la longitud m√°xima de las listas para definir el n√∫mero de columnas\n",
    "    max_length = max(len(d) for d in new_data)\n",
    "\n",
    "    # Generar los nombres de columna para el DataFrame resultante\n",
    "    new_columns = [f'Columna_{nombre_columna}_{i+1}' for i in range(max_length)]\n",
    "\n",
    "    # Crear un DataFrame de pandas a partir de los datos divididos y asignar nombres de columna\n",
    "    return pd.DataFrame(new_data, columns=new_columns)\n",
    "\n",
    "def log_MinMax(X):\n",
    "    \"\"\"\n",
    "    Aplica logaritmo y MinMaxScaler a un conjunto de datos.\n",
    "    \"\"\"\n",
    "    X = np.log1p(X)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def create_pipeline(algorithm, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Esta funci√≥n recibe un algoritmo de agrupaci√≥n de sklearn y un n√∫mero de clusters, y retorna un pipeline.\n",
    "    \n",
    "    Args:\n",
    "        algorithm (str or object): Algoritmo de clustering o su nombre.\n",
    "        n_clusters (int): N√∫mero de clusters.\n",
    "        \n",
    "    Returns:\n",
    "        sklearn.pipeline.Pipeline: Un pipeline que contiene el algoritmo de clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(algorithm, str):\n",
    "        if algorithm == 'KMeans':\n",
    "            algorithm = KMeans(n_clusters=n_clusters, random_state=99)\n",
    "        elif algorithm == 'SpectralClustering':\n",
    "            algorithm =  SpectralClustering(n_clusters=k, affinity='nearest_neighbors', eigen_solver='arpack', n_init=100)\n",
    "        elif algorithm == 'AgglomerativeClustering':\n",
    "            algorithm = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        elif algorithm == 'Birch':\n",
    "            algorithm = Birch(n_clusters=n_clusters)\n",
    "        else:\n",
    "            raise ValueError(\"El algoritmo no es compatible o no est√° implementado en la funci√≥n.\")\n",
    "    else:\n",
    "        algorithm.n_clusters = n_clusters\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('clustering', algorithm)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def convert_to_pdf(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Toma una imagen y la convierte a pdf.\n",
    "    \n",
    "    Args:   \n",
    "        input_path (str): Ruta de la imagen.\n",
    "        output_path (str): Ruta del pdf.\n",
    "    \n",
    "    Returns:\n",
    "        No retorna nada.\n",
    "    \"\"\"\n",
    "    \n",
    "    c = canvas.Canvas(output_path, pagesize=letter)\n",
    "    c.drawImage(input_path, 0, 0, letter[0]*0.8, letter[1], preserveAspectRatio=True, mask='auto')\n",
    "    c.save()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class Profiler:\n",
    "    def __init__(self, dataset):\n",
    "        # Inicializar el profiler\n",
    "        self.path = os.getcwd()\n",
    "        self.date = datetime.datetime.now().strftime(\"%d-%m-%Y\")\n",
    "        new_folder = os.path.join(self.path, f\"EDA_{self.date}\")\n",
    "        os.makedirs(new_folder, exist_ok=True)\n",
    "        self.path = new_folder\n",
    "        self.dataset = dataset\n",
    "        self.dataset_variables = None \n",
    "        self.dataset_clean = None\n",
    "        self.dataset_scale = None \n",
    "\n",
    "    def summarize(self, variables=None):\n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de caracterizar las variables del dataset.\n",
    "        \n",
    "        Args:\n",
    "            variables (list): Lista con las variables del dataset.\n",
    "            En caso de que no se especifique, se toman todas las variables del dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Archivo summary.txt con la caracterizaci√≥n de las variables. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        if variables is None:\n",
    "            variables = self.dataset.columns\n",
    "        \n",
    "        self.dataset_variables = self.dataset[variables]\n",
    "        \n",
    "        \n",
    "        # detectar anomalias\n",
    "        \n",
    "        # Vemos si hay alguna columna con datos no atomicos\n",
    "        no_atomicas = detectar_columnas_no_atomicas(self.dataset_variables)\n",
    "                \n",
    "        # detectamos outliers\n",
    "        columna_outliers = []\n",
    "        for col in self.dataset_variables.select_dtypes(include=['number']):\n",
    "            if detectar_outliers(self.dataset_variables[col]):\n",
    "                columna_outliers.append(col)\n",
    "        \n",
    "        # Se crea el archivo summary.txt\n",
    "        with open(os.path.join(self.path, \"summary.txt\"), \"w\") as summary:\n",
    "            # Se reporta el tipo de las variables\n",
    "            summary.write(\"Tipo de las variables:\\n\")\n",
    "            summary.write(str(self.dataset_variables.dtypes))\n",
    "            summary.write(\"\\n\\n\")\n",
    "            \n",
    "            # Se reporta el porcentaje de valores √∫nicos por variable\n",
    "            summary.write(\"Porcentaje de valores √∫nicos por variable:\\n\")\n",
    "            summary.write(str(self.dataset_variables.nunique() / len(self.dataset_variables) * 100))\n",
    "            summary.write(\"\\n\\n\")\n",
    "            \n",
    "            # Se reporta el porcentaje de valores nulos por variable\n",
    "            summary.write(\"Porcentaje de valores nulos por variable:\\n\")\n",
    "            summary.write(str(self.dataset_variables.isnull().sum() / len(self.dataset_variables) * 100))\n",
    "            summary.write(\"\\n\\n\")\n",
    "            \n",
    "            # Reporte de anomalias\n",
    "            summary.write(\"Reporte de anomalias:\\n\")\n",
    "            if len(no_atomicas) > 0:\n",
    "                summary.write(f\"Las columnas {no_atomicas} tienen datos no at√≥micos.\\n\")\n",
    "            else:\n",
    "                summary.write(\"No se encontraron columnas con datos no at√≥micos.\\n\")\n",
    "            summary.write(\"\\n\\n\")\n",
    "            if len(columna_outliers) > 0:\n",
    "                summary.write(f\"Las columnas {columna_outliers} tienen outliers.\\n\")\n",
    "            else:\n",
    "                summary.write(\"No se encontraron columnas numericas con outliers.\\n\")\n",
    "            summary.write(\"\\n\\n\")\n",
    "            \n",
    "            \n",
    "            # En caso de ser variable num√©rica, se reporta la media, mediana, desviaci√≥n est√°ndar, m√≠nimo y m√°ximo\n",
    "            # percentiles 25, 50, 75 y 100, adem√°s los porcentajes de valores cero, negativos y outliers.\n",
    "            # En caso de encontrar anomal√≠as, se reporta en un archivo de texto.\n",
    "            # for sobre columnas numericas\n",
    "            for col in self.dataset_variables.select_dtypes(include=['number']):\n",
    "                \n",
    "                \n",
    "                summary.write(f\"Columna: {col}\\n\")\n",
    "                summary.write(f\"Porcentaje de nulos: {self.dataset_variables[col].isnull().sum()/len(self.dataset_variables[col])}\\n\")\n",
    "                summary.write(f\"Porcentaje de negativos: {np.sum(self.dataset_variables[col]<0)/len(self.dataset_variables[col])}\\n\")\n",
    "                summary.write(f\"Porcentaje de outliers: {np.sum(np.abs(ss.zscore(self.dataset_variables[col].dropna()))>3)/len(self.dataset_variables[col])}\\n\")\n",
    "                \n",
    "                # reportamos estadisticas: valor minimo, maximo, promedio, percentiles 25, 50 y 75 y 100\n",
    "                print('Minimo: ', dataset[col].min())\n",
    "                print('Maximo: ', dataset[col].max())\n",
    "                print('Promedio: ', dataset[col].mean())\n",
    "                print('Percentil 25: ', dataset[col].quantile(0.25))\n",
    "                print('Percentil 50: ', dataset[col].quantile(0.5))\n",
    "                print('Percentil 75: ', dataset[col].quantile(0.75))\n",
    "                print('Percentil 100: ', dataset[col].quantile(1))\n",
    "                summary.write(f\"Minimo: {self.dataset_variables[col].min()}\\n\")\n",
    "                summary.write(f\"Maximo: {self.dataset_variables[col].max()}\\n\")\n",
    "                summary.write(f\"Promedio: {self.dataset_variables[col].mean()}\\n\")\n",
    "                summary.write(f\"Percentil 25: {self.dataset_variables[col].quantile(0.25)}\\n\")\n",
    "                summary.write(f\"Percentil 50: {self.dataset_variables[col].quantile(0.5)}\\n\")\n",
    "                summary.write(f\"Percentil 75: {self.dataset_variables[col].quantile(0.75)}\\n\")\n",
    "                summary.write(f\"Percentil 100: {self.dataset_variables[col].quantile(1)}\\n\")\n",
    "                summary.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "            \n",
    "# Falta levantar la alerta de anomalias\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def plot_vars(self, variables=None, N = 10, useCleanData = False):\n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de graficar la distribuci√≥n e interacciones de las variables del dataset.\n",
    "\n",
    "        Args:\n",
    "            variables (list): Lista con las variables del dataset.\n",
    "            En caso de que no se especifique, se toman todas las variables del dataset.\n",
    "\n",
    "            N (int): Indica el top N de categor√≠as a graficar en las variables categ√≥ricas.\n",
    "\n",
    "            useCleanData (bool): Indica si se usar√° el dataset limpio o no.\n",
    "        \n",
    "        Returns:\n",
    "            Carpeta plots con los gr√°ficos de las variables. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        # Se crea la carpeta plots\n",
    "        os.makedirs(os.path.join(self.path, \"plots\"), exist_ok=True)\n",
    "\n",
    "        if useCleanData:\n",
    "            if self.dataset_clean is None:\n",
    "                self.clean_data()\n",
    "            self.dataset_variables = self.dataset_clean\n",
    "        else:\n",
    "            if variables is None:\n",
    "                variables = self.dataset.columns\n",
    "            self.dataset_variables = self.dataset[variables]\n",
    "\n",
    "        numerical_columns = self.dataset_variables.select_dtypes(include=\"number\").columns\n",
    "        categorical_columns = self.dataset_variables.select_dtypes(include=\"object\").columns\n",
    "        \n",
    "        # Se grafica la distribuci√≥n de densidad de las variables num√©ricas\n",
    "        for variable in numerical_columns:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            self.dataset_variables[variable].plot(kind=\"density\")\n",
    "            plt.title(f\"Distribuci√≥n de densidad de la variable {variable}\")\n",
    "            plt.xlabel(variable)\n",
    "            plt.ylabel(\"Densidad\")\n",
    "            plt.savefig(os.path.join(self.path, \"plots\", f\"{variable}.pdf\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Se grafica la correlaci√≥n entre las variables num√©ricas\n",
    "        # Con titulo y anotaciones de los valores de correlacion en el grafico \n",
    "        corr_matrix = self.dataset_variables.select_dtypes(include=\"number\").corr()\n",
    "        sns.heatmap(corr_matrix, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True).set_title(\"Correlaci√≥n entre las variables num√©ricas\")\n",
    "        plt.savefig(os.path.join(self.path, \"plots\", \"correlation.pdf\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Se grafica el histograma de las top N categor√≠as de las variables categ√≥ricas\n",
    "        for variable in categorical_columns:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            self.dataset_variables[variable].value_counts()[:N].plot(kind=\"bar\")\n",
    "            plt.title(f\"Histograma de las top {N} categor√≠as de la variable {variable}\")\n",
    "            plt.xlabel(variable)\n",
    "            plt.ylabel(\"Frecuencia\")\n",
    "            plt.savefig(os.path.join(self.path, \"plots\", f\"{variable}_hist.pdf\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "        \n",
    "        # Se grafica el coeficiente V de Cramer entre las variables categ√≥ricas\n",
    "        num_cols = len(categorical_columns)\n",
    "        cramer_matrix = np.zeros((num_cols, num_cols))\n",
    "        for i, col1 in enumerate(categorical_columns):\n",
    "            for j, col2 in enumerate(categorical_columns):\n",
    "                if i!=j:\n",
    "                    try:\n",
    "                        result_cramer_v = cramer_v(self.dataset_variables[col1], self.dataset_variables[col2])\n",
    "                        \n",
    "                        # Almacena el valor en la matriz\n",
    "                        cramer_matrix[i, j] = result_cramer_v\n",
    "                    except:\n",
    "                        # En caso de error, se almacena un cero\n",
    "                        cramer_matrix[i, j] = None\n",
    "                        print(f\"Error al calcular el coeficiente V de Cramer entre {col1} y {col2}\")\n",
    "                \n",
    "                else:\n",
    "                    cramer_matrix[i, j] = 1\n",
    "\n",
    "        sns.heatmap(cramer_matrix, xticklabels=categorical_columns, yticklabels=categorical_columns, annot=True).set_title(\"Coeficiente V de Cramer entre las variables categ√≥ricas\")\n",
    "        plt.savefig(os.path.join(self.path, \"plots\", \"cramer_v.pdf\"), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "    def clean_data(self, variables=None, nulos = None):\n",
    "        \"\"\"\n",
    "        - Crear la carpeta `EDA_fecha/clean_data`\n",
    "        - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "        - Drop de valores duplicados\n",
    "        - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "            - Drop de valores nulos\n",
    "            - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "            - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "        - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "            - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "            - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "        - Deber√≠an usar `FunctionTransformer`.\n",
    "        - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "        \n",
    "        Returns:\n",
    "            Carpeta clean_data con el dataset limpio. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Se crea la carpeta clean_data\n",
    "        os.makedirs(os.path.join(self.path, \"clean_data\"), exist_ok=True)\n",
    "        \n",
    "        # filtramos por las variables de interes\n",
    "        if variables is None:\n",
    "            variables = self.dataset.columns\n",
    "        self.dataset_variables = self.dataset[variables]\n",
    "        \n",
    "        \n",
    "        # Drop de valores duplicados\n",
    "        self.dataset_variables.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # Separar columna no atomica utilizando FuntionTransformer\n",
    "        # obtenemos las columnas no atomicas\n",
    "        no_atomicas = detectar_columnas_no_atomicas(self.dataset_variables)\n",
    "            \n",
    " \n",
    "        # Creamos el transformer\n",
    "        transformer = FunctionTransformer(transformar_columna)\n",
    "        \n",
    "        # Aplicamos el transformer a las columnas no atomicas\n",
    "        for columna in no_atomicas:\n",
    "            # remplazamos la columna atomica por las nuevas columnas\n",
    "            self.dataset_variables = pd.concat([self.dataset_variables.drop(columna, axis=1), transformer.transform(self.dataset_variables[columna])], axis=1)\n",
    "            \n",
    "            \n",
    "        # Tratamiento de valores nulos\n",
    "        if nulos == None or nulos == 'drop': # si no se selecciona tratamiento realizamos drop\n",
    "            self.dataset_variables.dropna(inplace=True)\n",
    "            \n",
    "        elif nulos == 'fillna': # si se selecciona imputar con fillna\n",
    "            self.dataset_variables.fillna(self.dataset_variables.mean(), inplace=True)\n",
    "        elif nulos == 'interpolate':\n",
    "            self.dataset_variables.interpolate(method='linear', inplace=True)\n",
    "        \n",
    "        # Almacenamos el dataset limpio en la variable de la clase\n",
    "        self.dataset_clean = self.dataset_variables\n",
    "            \n",
    "        # Guardamos el dataset limpio\n",
    "        self.dataset_variables.to_csv(os.path.join(self.path, \"clean_data\", \"data.csv\"), index=False)        \n",
    "    \n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "    def scale(self, scaler_categorico=None):\n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de escalar las variables del dataset. Para variables numericas se aplica logaritmo y MinMaxScaler.\n",
    "        Para variables categoricas se aplica el scaler_categorico.\n",
    "\n",
    "        Args:\n",
    "            scaler_categorico (sklearn.preprocessing): T√©cnica de escalamiento para las variables categ√≥ricas.\n",
    "            En caso de que no se especifique, se toma el escalamiento por defecto de sklearn.\n",
    "        \n",
    "        Returns:\n",
    "            Carpeta scale con el dataset escalado. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        # Se crea la carpeta scale\n",
    "        os.makedirs(os.path.join(self.path, \"scale\"), exist_ok=True)\n",
    "        \n",
    "\n",
    "        if self.dataset_variables is None:\n",
    "            self.dataset_variables = self.dataset\n",
    "            \n",
    "\n",
    "        # En caso de que scaler_categorico sea None, se toma el escalamiento por defecto de sklearn\n",
    "        if scaler_categorico is None:\n",
    "            scaler_categorico = OrdinalEncoder()\n",
    "        \n",
    "        # Seleccionamos las columnas numericas\n",
    "        num_cols = self.dataset_variables.select_dtypes(include=\"number\").columns\n",
    "        # Seleccionamos las columnas categoricas\n",
    "        cat_cols = self.dataset_variables.select_dtypes(include=\"object\").columns\n",
    "        \n",
    "        # Creamos el ColumnTransformer que procesa los datos numericos y categoricos\n",
    "        ct = ColumnTransformer([\n",
    "            ('log_and_minmax', FunctionTransformer(func=log_MinMax), num_cols),\n",
    "            ('categoric', scaler_categorico, cat_cols)\n",
    "        ])\n",
    "\n",
    "        # Aplicamos el ColumnTransformer\n",
    "        ct.fit_transform(self.dataset_variables)\n",
    "\n",
    "        # Se escala el dataset\n",
    "        self.dataset_variables = pd.DataFrame(ct.transform(self.dataset_variables), columns=num_cols.tolist()+cat_cols.tolist())\n",
    "\n",
    "        # Se guarda el dataset escalado en la variable de la clase\n",
    "        self.dataset_scale = self.dataset_variables\n",
    "        \n",
    "        # Guardamos el dataset escalado\n",
    "        pd.DataFrame(self.dataset_variables).to_csv(os.path.join(self.path, \"scale\", \"scaled_features.csv\"), index=False)\n",
    "\n",
    "                \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# Make clusters\n",
    "\n",
    "    def make_clusters(self, algoritmo = None): \n",
    "        \n",
    "        \"\"\"\n",
    "            - Crear la carpeta `EDA_fecha/clusters`\n",
    "            - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "            - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "            - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "            - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "            - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "            - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "            - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de agrupar las variables del dataset, seg√∫n el algoritmo especificado. \n",
    "        \n",
    "        Args:\n",
    "            str (sklearn.cluster): str del nombre del algoritmo de clustering.\n",
    "            En caso de que no se especifique, se toma el algoritmo KMeans de sklearn.\n",
    "            \n",
    "            Algoritmos permitidos: KMeans, Birch, SpectralClustering\n",
    "            Pendiente: agregar AgglomerativeClustering\n",
    "        \n",
    "        Return: \n",
    "            Carpeta clusters con el dataset agrupado. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Se crea la carpeta clusters\n",
    "        os.makedirs(os.path.join(self.path, \"clusters\"), exist_ok=True)\n",
    "        \n",
    "       \n",
    "        # Realizamos el preprocesamiento de la data, se limpia y se escala\n",
    "    \n",
    "        # reinicializamos el dataset variables para no tener problemas de ejecuciones previas\n",
    "        self.dataset_variables =None\n",
    "        # clean_data\n",
    "        self.clean_data()\n",
    "        # notificamos que se ha limpiado la data\n",
    "        print(\"Se ha limpiado la data\")\n",
    "        # scale\n",
    "        self.scale()\n",
    "        # notificamos que se ha escalado la data\n",
    "        print(\"Se ha escalado la data\")\n",
    "\n",
    "\n",
    "\n",
    "        # recuperamos el dataset escalado\n",
    "        scale_data = pd.read_csv(os.path.join(self.path, \"scale\", \"scaled_features.csv\"))\n",
    "        # notificamos que se realizara el metodo del codo\n",
    "        print(\"Se realizara el metodo del codo...\")\n",
    "        print(\"algoritmo escogido: \", algoritmo)\n",
    "        print(\"Esto puede tardar unos minutos\")\n",
    "        \n",
    "        \n",
    "        if algoritmo is None or algoritmo == 'KMeans':\n",
    "            algoritmo = 'KMeans'\n",
    "            # realizamos el metodo del codo para determinar el numero de clusters con kmeans\n",
    "            intertias = [\n",
    "                [i, \n",
    "                Pipeline(steps=[\n",
    "                    (\"Kmeans\",KMeans(n_clusters=i, random_state=99))]).fit(scale_data).named_steps[\"Kmeans\"].inertia_]\n",
    "                for i in range(1, 20)\n",
    "            ]\n",
    "\n",
    "        elif algoritmo == 'SpectralClustering':\n",
    "\n",
    "            # Generar datos de muestra\n",
    "            X = scale_data\n",
    "\n",
    "            # Inicializar listas para almacenar resultados\n",
    "            silhouette_scores = []\n",
    "            num_clusters = []\n",
    "\n",
    "            # Iterar sobre un rango de posibles valores de k\n",
    "            for k in range(2, 11):\n",
    "                # Crear objeto SpectralClustering\n",
    "                sc = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', eigen_solver='arpack', n_init=100)\n",
    "\n",
    "                # Ajustar y predecir\n",
    "                labels = sc.fit_predict(X)\n",
    "\n",
    "                # Calcular el score de silueta\n",
    "                silhouette_avg = silhouette_score(X, labels)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "                num_clusters.append(k)\n",
    "                \n",
    "            plt.plot(num_clusters, silhouette_scores, marker='o')\n",
    "            plt.xlabel('Number of clusters')\n",
    "            plt.ylabel('Silhouette Score')\n",
    "            plt.title('Silhouette Score for Spectral Clustering')\n",
    "            plt.show()\n",
    "            \n",
    "            # guardamos el grafico\n",
    "            plt.savefig(os.path.join(self.path, \"clusters\", \"slihouette.pdf\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "               \n",
    "            silhouette_scores = pd.DataFrame(silhouette_scores, columns=[\"Silhouette Score\"])\n",
    "            num_clusters = pd.DataFrame(num_clusters, columns=[\"Number of clusters\"])\n",
    "            silhouette_scores = pd.concat([num_clusters, silhouette_scores], axis=1)\n",
    "            num_clusters = silhouette_scores[silhouette_scores[\"Silhouette Score\"] == silhouette_scores[\"Silhouette Score\"].max()][\"Number of clusters\"].values[0]\n",
    "            num_clusters\n",
    "            \n",
    "        elif algoritmo == 'AgglomerativeClustering':\n",
    "            intertias = [\n",
    "                [i, \n",
    "                Pipeline(steps=[\n",
    "                    (\"AgglomerativeClustering\",AgglomerativeClustering(n_clusters=i))]).fit(scale_data).named_steps[\"AgglomerativeClustering\"].inertia_]\n",
    "                for i in range(1, 20)\n",
    "            ]\n",
    "        elif algoritmo == 'Birch':\n",
    "            sample_data = scale_data.sample(frac=0.1, random_state=99)\n",
    "\n",
    "            silhouette_scores = []\n",
    "            for i in range(2, 20):\n",
    "                birch = Birch(n_clusters=i)\n",
    "                birch.fit(sample_data)\n",
    "                labels = birch.predict(sample_data)\n",
    "                silhouette_avg = silhouette_score(sample_data, labels)\n",
    "                silhouette_scores.append([i, silhouette_avg])\n",
    "            intertias = silhouette_scores\n",
    "        else:\n",
    "            raise ValueError(\"El algoritmo no es compatible o no est√° implementado en la funci√≥n.\")\n",
    "        \n",
    "        \n",
    "        if algoritmo == 'KMeans' or  algoritmo == 'Birch':\n",
    "            # Creamos un DataFrame con los resultados del m√©todo del codo\n",
    "            intertias = pd.DataFrame(intertias, columns=[\"n¬∞ clusters\", \"inertia\"])\n",
    "            \n",
    "\n",
    "            # Calculamos el numero de clusters optimo\n",
    "            umbral = -0.5 * intertias[\"inertia\"].std()  # ajustable\n",
    "            n_clusters = intertias[intertias[\"inertia\"].diff() < umbral].iloc[1, 0]\n",
    "\n",
    "            # Graficamos el metodo del codo\n",
    "            # Gr√°fico del m√©todo del codo\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(intertias[\"n¬∞ clusters\"], intertias[\"inertia\"], marker=\"o\")\n",
    "            plt.axvline(x=n_clusters, linestyle=\"--\", color=\"red\", label=\"N√∫mero √≥ptimo de clusters\")\n",
    "            plt.xlabel(\"N√∫mero de clusters\")\n",
    "            plt.ylabel(\"Inercia\")\n",
    "            plt.title(\"M√©todo del Codo\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Guardamos el grafico\n",
    "            plt.savefig(os.path.join(self.path, \"clusters\", \"codo.png\"), bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Se transforma la imagen png a pdf\n",
    "            convert_to_pdf(os.path.join(profiler.path, \"clusters\", \"codo.png\"), os.path.join(self.path, \"clusters\", \"codo.pdf\"))\n",
    "            \n",
    "            # Se crea el pipeline con el algoritmo de clustering y el numero de clusters optimo\n",
    "            pipeline = create_pipeline(algoritmo, n_clusters=n_clusters)\n",
    "            \n",
    "            # Entrenamos el pipeline\n",
    "            pipeline.fit(scale_data)\n",
    "        \n",
    "            # Predecimos las etiquetas de los clusters\n",
    "            scale_data['cluster'] = pipeline.predict(scale_data)\n",
    "        \n",
    "        elif algoritmo == 'SpectralClustering':\n",
    "            \n",
    "            # Se crea el pipeline con el algoritmo de clustering y el numero de clusters optimo\n",
    "            pipeline = create_pipeline(algoritmo, n_clusters=num_clusters)\n",
    "        \n",
    "            # Predecimos las etiquetas de los clusters\n",
    "            scale_data['cluster'] = sc.fit_predict(scale_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # PCA\n",
    "        # Creamos el PCA\n",
    "        pca = PCA(n_components=2)\n",
    "\n",
    "        # Ajustamos el PCA\n",
    "        pca.fit(scale_data)\n",
    "\n",
    "        # Transformamos los datos\n",
    "        pca_data = pca.transform(scale_data)\n",
    "\n",
    "        # Creamos un DataFrame con los datos transformados\n",
    "        pca_data = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "        # Agregamos la columna de los clusters\n",
    "        pca_data['cluster'] = scale_data['cluster']\n",
    "\n",
    "        # Graficamos los datos\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.scatterplot(data=pca_data, x='PC1', y='PC2', hue='cluster', palette='Set2')\n",
    "        plt.title(\"Proyecci√≥n de los datos en 2 dimensiones\")\n",
    "        \n",
    "        # Guardamos el grafico\n",
    "        plt.savefig(os.path.join(self.path, \"clusters\", \"pca.pdf\"), bbox_inches='tight')\n",
    "        \n",
    "        # Guardamos el dataset con los clusters\n",
    "        scale_data.to_csv(os.path.join(self.path, \"clusters\", \"data_clusters.csv\"), index=False)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def detect_anomalies(self, algoritmo = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de detectar anomal√≠as en los datos. \n",
    "        \n",
    "        Args:\n",
    "            algoritmo [objeto] (sklearn.cluster): Algoritmo de clustering.\n",
    "            En caso de que no se especifique, se toma el algoritmo IsolationForest de sklearn.\n",
    "        \n",
    "        Return: \n",
    "            Carpeta anomalies con el dataset con las anomal√≠as detectadas. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Se crea la carpeta anomalies\n",
    "        os.makedirs(os.path.join(self.path, \"anomalies\"), exist_ok=True)\n",
    "        \n",
    "        # Realizamos el preprocesamiento de la data, se limpia y se escala\n",
    "    \n",
    "        # reinicializamos el dataset variables para no tener problemas de ejecuciones previas\n",
    "        self.dataset_variables =None\n",
    "        \n",
    "        # clean_data\n",
    "        self.clean_data()\n",
    "        \n",
    "        # notificamos que se ha limpiado la data\n",
    "        print(\"Se ha limpiado la data\")\n",
    "        \n",
    "        # scale\n",
    "        self.scale()\n",
    "        \n",
    "        # notificamos que se ha escalado la data\n",
    "        print(\"Se ha escalado la data\")\n",
    "\n",
    "        # recuperamos el dataset escalado\n",
    "        scale_data = pd.read_csv(os.path.join(self.path, \"scale\", \"scaled_features.csv\"))\n",
    "        \n",
    "        # En caso de que algoritmo sea None, se toma el algoritmo IsolationForest de sklearn\n",
    "        if algoritmo is None:\n",
    "            algoritmo = IsolationForest(random_state=99)\n",
    "        # Creamos el pipeline con el algoritmo de clustering y el numero de clusters optimo\n",
    "        pipeline = create_pipeline(algoritmo)\n",
    "\n",
    "        # Entrenamos el pipeline\n",
    "        #pipeline.fit(scale_data.sample(frac=0.2, random_state=99))\n",
    "        \n",
    "        # Predecimos las etiquetas de los clusters\n",
    "        scale_data['anomaly'] = pipeline.fit_predict(scale_data)\n",
    "\n",
    "        # PCA\n",
    "        # Creamos el PCA\n",
    "        pca = PCA(n_components=2)\n",
    "\n",
    "        # Ajustamos el PCA\n",
    "        pca.fit(scale_data)\n",
    "\n",
    "        # Transformamos los datos\n",
    "        pca_data = pca.transform(scale_data)\n",
    "\n",
    "        # Creamos un DataFrame con los datos transformados\n",
    "        pca_data = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "        # Agregamos la columna de los clusters\n",
    "        pca_data['anomaly'] = scale_data['anomaly']\n",
    "\n",
    "        # Graficamos los datos\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.scatterplot(data=pca_data, x='PC1', y='PC2', hue='anomaly', palette='Set2')\n",
    "        plt.title(\"Proyecci√≥n de los datos en 2 dimensiones de las anomalias\")\n",
    "        \n",
    "        # Guardamos el grafico\n",
    "        plt.savefig(os.path.join(self.path, \"anomalies\", \"plot_anomalies.pdf\"), bbox_inches='tight')\n",
    "\n",
    "        # Guardamos el dataset con las anomal√≠as\n",
    "        scale_data.to_csv(os.path.join(self.path, \"anomalies\", \"data_anomalies.csv\"), index=False)\n",
    "\n",
    "       \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def profile(self, variables=None, N = 10, scaler_categorico=None, nulos = None, algoritmo_cluster = None, algoritmo_anomalies = None):\n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de ejecutar todos los m√©todos anteriores.\n",
    "        \n",
    "        Args:\n",
    "            variables (list): Lista con las variables del dataset.\n",
    "            En caso de que no se especifique, se toman todas las variables del dataset.\n",
    "\n",
    "            N (int): Indica el top N de categor√≠as a graficar en las variables categ√≥ricas.\n",
    "\n",
    "            scaler_categorico (sklearn.preprocessing): T√©cnica de escalamiento para las variables categ√≥ricas.\n",
    "            En caso de que no se especifique, se toma el escalamiento por defecto de sklearn.\n",
    "\n",
    "            nulos (str): T√©cnica para tratar los valores nulos.\n",
    "            En caso de que no se especifique, se toma el drop de valores nulos.\n",
    "\n",
    "            algoritmo_cluster (str): Nombre del algoritmo de clustering.\n",
    "            En caso de que no se especifique, se toma el algoritmo KMeans de sklearn.\n",
    "            \n",
    "            algoritmo_anomalies (sklearn.cluster): Algoritmo de clustering.\n",
    "            En caso de que no se especifique, se toma el algoritmo KMeans de sklearn.\n",
    "        \n",
    "        Returns:\n",
    "            Carpeta EDA_fecha con todos los resultados de los m√©todos anteriores. Se almacena\n",
    "            en la carpeta que se crea en el constructor.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Se crea la carpeta EDA_fecha\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "        \n",
    "        # Se ejecutan los m√©todos anteriores\n",
    "        self.summarize(variables)\n",
    "        self.clean_data(variables, nulos)\n",
    "        self.plot_vars(variables, N, useCleanData=True)\n",
    "        self.scale(scaler_categorico)\n",
    "        self.make_clusters(algoritmo_cluster)\n",
    "        self.detect_anomalies(algoritmo_anomalies)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------   \n",
    "\n",
    "    def clearGarbage(self):\n",
    "        \"\"\"\n",
    "        Este m√©todo se encarga de eliminar las carpetas/archivos creados/as por la clase Profiler.\n",
    "        \n",
    "        Args:\n",
    "            No recibe argumentos.\n",
    "        \n",
    "        Returns:\n",
    "            No retorna nada.\n",
    "        \"\"\"\n",
    "        # Se elimina la carpeta EDA_fecha aun que no est√© vac√≠o\n",
    "        shutil.rmtree(self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = Profiler(dataset)\n",
    "#profiler.clean_data()\n",
    "#profiler.scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Benjamin Nicholas\\AppData\\Local\\Temp\\ipykernel_51308\\1634637160.py:28: UserWarning: FigureCanvasPdf is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\"\"\" import matplotlib\n",
    "matplotlib.use('pdf')\n",
    "\n",
    "scale_data = pd.read_csv(os.path.join(profiler.path, \"scale\", \"scaled_features.csv\"))\n",
    "algoritmo = 'KMeans'\n",
    "# realizamos el metodo del codo para determinar el numero de clusters con kmeans\n",
    "intertias = [\n",
    "    [i, \n",
    "    Pipeline(steps=[\n",
    "        (\"Kmeans\",KMeans(n_clusters=i, random_state=99))]).fit(scale_data).named_steps[\"Kmeans\"].inertia_]\n",
    "    for i in range(1, 20)\n",
    "            ]\n",
    "intertias = pd.DataFrame(intertias, columns=[\"n¬∞ clusters\", \"inertia\"])\n",
    "\n",
    "# Calculamos el numero de clusters optimo\n",
    "umbral = -0.5 * intertias[\"inertia\"].std()  # ajustable\n",
    "n_clusters = intertias[intertias[\"inertia\"].diff() < umbral].iloc[1, 0]\n",
    "\n",
    "# Graficamos el metodo del codo\n",
    "# Gr√°fico del m√©todo del codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(intertias[\"n¬∞ clusters\"], intertias[\"inertia\"], marker=\"o\")\n",
    "plt.axvline(x=n_clusters, linestyle=\"--\", color=\"red\", label=\"N√∫mero √≥ptimo de clusters\")\n",
    "plt.xlabel(\"N√∫mero de clusters\")\n",
    "plt.ylabel(\"Inercia\")\n",
    "plt.title(\"M√©todo del Codo\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Guardamos el grafico\n",
    "plt.savefig(os.path.join(profiler.path, \"clusters\", \"codo.png\"), bbox_inches='tight')\n",
    "plt.close() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimo:  5\n",
      "Maximo:  135551\n",
      "Promedio:  68299.38171289465\n",
      "Percentil 25:  34642.25\n",
      "Percentil 50:  68245.0\n",
      "Percentil 75:  102050.25\n",
      "Percentil 100:  135551.0\n",
      "Minimo:  1896\n",
      "Maximo:  2016\n",
      "Promedio:  1978.5050899970493\n",
      "Percentil 25:  1960.0\n",
      "Percentil 50:  1988.0\n",
      "Percentil 75:  2004.0\n",
      "Percentil 100:  2016.0\n"
     ]
    }
   ],
   "source": [
    "profiler.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha limpiado la data\n",
      "Se ha escalado la data\n",
      "Se realizara el metodo del codo...\n",
      "algoritmo escogido:  KMeans\n",
      "Esto puede tardar unos minutos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Benjamin Nicholas\\AppData\\Local\\Temp\\ipykernel_51308\\711779810.py:591: UserWarning: FigureCanvasPdf is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "profiler.make_clusters(algoritmo = 'KMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha limpiado la data\n",
      "Se ha escalado la data\n"
     ]
    }
   ],
   "source": [
    "profiler = Profiler(dataset)\n",
    "profiler.detect_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimo:  5\n",
      "Maximo:  135551\n",
      "Promedio:  68299.38171289465\n",
      "Percentil 25:  34642.25\n",
      "Percentil 50:  68245.0\n",
      "Percentil 75:  102050.25\n",
      "Percentil 100:  135551.0\n",
      "Minimo:  1896\n",
      "Maximo:  2016\n",
      "Promedio:  1978.5050899970493\n",
      "Percentil 25:  1960.0\n",
      "Percentil 50:  1988.0\n",
      "Percentil 75:  2004.0\n",
      "Percentil 100:  2016.0\n",
      "Se ha limpiado la data\n",
      "Se ha escalado la data\n",
      "Se realizara el metodo del codo...\n",
      "algoritmo escogido:  None\n",
      "Esto puede tardar unos minutos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\Benjamin Nicholas\\AppData\\Local\\Temp\\ipykernel_51308\\2851914206.py:598: UserWarning: FigureCanvasPdf is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "b:\\GITPROJECTS\\LAB MDS\\pjenv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha limpiado la data\n",
      "Se ha escalado la data\n"
     ]
    }
   ],
   "source": [
    "profiler.profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.clearGarbage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
